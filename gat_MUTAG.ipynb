{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaakitori/negima/blob/main/gat_MUTAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBeK8bWc4a_9"
      },
      "source": [
        "# GATの実装\n",
        "Graph Attention Networks（GAT）を実装\n",
        "多数のグラフを持つデータセットを訓練データに使い、ミニバッチ法により学習を行う\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h7BA67Ed5wT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d959364-4172-4ba5-e282-f3b5aec1c878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLxU1W19uDBJ"
      },
      "source": [
        "Googleドライブ上のパスを指定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnqZq1W7IkzR"
      },
      "outputs": [],
      "source": [
        "dir_name = \"Library/GNN\"  # 好きなパスを設定してください\n",
        "package_path = \"/content/drive/MyDrive/\" + dir_name + \"/packages/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztIkOM_N5Bu7"
      },
      "source": [
        "## PyTorch Geometricのインストール\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLrzxL39qGie"
      },
      "outputs": [],
      "source": [
        "# !pip install --no-cache-dir torch-geometric torch-sparse torch-scatter -t $package_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7vHsBUGgb7u"
      },
      "source": [
        "Google Driveに保存したパッケージをシステムに追加  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx1fjOrB-ZPR"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(package_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZeLe5ktmDCG"
      },
      "source": [
        "## データセットの読み込み\n",
        "TUDatasetから、188のグラフが含まれるデータセット「MUTAG」を読み込む\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlyKILeKUskH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8bfae14-036d-41af-87eb-a957b252bcf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "dataset = TUDataset(root=\"/tmp/MUTAG\", name=\"MUTAG\")\n",
        "\n",
        "dataset = dataset.shuffle()  # データセットをシャッフル\n",
        "dataset_train = dataset[:140]  # 訓練用データセット\n",
        "dataset_test = dataset[140:]  # テスト用データセット\n",
        "\n",
        "batch_size = 64  # バッチサイズ\n",
        "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2s4JTtLh_cO"
      },
      "source": [
        "## モデルの構築\n",
        "GAT層の実装には、`GATConv()`を利用\n",
        "```\n",
        "GATConv(入力の特徴量数, 出力の特徴量数, Multi-head Attentionの数)\n",
        "```  \n",
        "https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GATConv  \n",
        "GATConv層を2つ重ね、全結合層の前にニューロンをランダムに削除する「ドロップアウト」を導入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGMeG-bgldbu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace5f5ab-da81-4f7f-89e7-4f0effbb0a77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GAT(\n",
              "  (gat1): GATConv(7, 64, heads=32)\n",
              "  (gat2): GATConv(2048, 64, heads=32)\n",
              "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "n_h = 64  # 中間層における特徴量の数\n",
        "n_head = 32\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.gat1 = GATConv(dataset.num_node_features,\n",
        "                            n_h,\n",
        "                            heads=n_head) # 特に設定しなければ各ヘッドの出力は結合される\n",
        "        self.gat2 = GATConv(n_h*n_head, # 1層目のGATConvの出力の数（中間層における特徴量の数×ヘッドの数）\n",
        "                            n_h,\n",
        "                            heads=n_head)\n",
        "        self.fc = nn.Linear(n_h*n_head, dataset.num_classes)  # 全結合層\n",
        "\n",
        "        self.relu = nn.ReLU()  # ReLU\n",
        "        self.dropout = nn.Dropout(p=0.5)  # ドロップアウト:(p=ドロップアウト率)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x\n",
        "        edge_index = data.edge_index\n",
        "        batch = data.batch\n",
        "\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = self.relu(x)\n",
        "        x = self.gat2(x, edge_index)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # 全てのノードで各特徴量の平均をとる\n",
        "        # ノードの次元をつぶす＋ヘッドの数分、各ノードに特徴量があるのでそれも含め平均とる\n",
        "        x = global_mean_pool(x, batch)  # (バッチサイズ, 特徴量の数)に変換\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "net = GAT()\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP9O1HhFikx-"
      },
      "source": [
        "## 学習"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデル評価用の関数を用意"
      ],
      "metadata": {
        "id": "RvQn3Apg_Ig_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(loader):\n",
        "    correct = 0  # 正解数\n",
        "\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out = net(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += int((pred == data.y).sum())\n",
        "\n",
        "    return correct/len(loader.dataset)  # 正解率"
      ],
      "metadata": {
        "id": "vwqxhW52_Phq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "訓練データを使い、モデルを訓練"
      ],
      "metadata": {
        "id": "cLmtIpUR_Olw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfv35TJsnNcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42f15d7d-8d3b-41ff-b9ff-ce99db9c62fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 acc_train: 63.57142857142857% acc_test: 75.0%\n",
            "Epoch: 1 acc_train: 63.57142857142857% acc_test: 75.0%\n",
            "Epoch: 2 acc_train: 63.57142857142857% acc_test: 75.0%\n",
            "Epoch: 3 acc_train: 63.57142857142857% acc_test: 75.0%\n",
            "Epoch: 4 acc_train: 63.57142857142857% acc_test: 75.0%\n",
            "Epoch: 5 acc_train: 63.57142857142857% acc_test: 75.0%\n",
            "Epoch: 6 acc_train: 63.57142857142857% acc_test: 75.0%\n",
            "Epoch: 7 acc_train: 65.0% acc_test: 75.0%\n",
            "Epoch: 8 acc_train: 66.42857142857143% acc_test: 79.16666666666666%\n",
            "Epoch: 9 acc_train: 71.42857142857143% acc_test: 83.33333333333334%\n",
            "Epoch: 10 acc_train: 71.42857142857143% acc_test: 85.41666666666666%\n",
            "Epoch: 11 acc_train: 70.71428571428572% acc_test: 81.25%\n",
            "Epoch: 12 acc_train: 71.42857142857143% acc_test: 83.33333333333334%\n",
            "Epoch: 13 acc_train: 71.42857142857143% acc_test: 79.16666666666666%\n",
            "Epoch: 14 acc_train: 68.57142857142857% acc_test: 81.25%\n",
            "Epoch: 15 acc_train: 71.42857142857143% acc_test: 83.33333333333334%\n",
            "Epoch: 16 acc_train: 73.57142857142858% acc_test: 85.41666666666666%\n",
            "Epoch: 17 acc_train: 72.85714285714285% acc_test: 83.33333333333334%\n",
            "Epoch: 18 acc_train: 71.42857142857143% acc_test: 85.41666666666666%\n",
            "Epoch: 19 acc_train: 72.14285714285714% acc_test: 83.33333333333334%\n",
            "Epoch: 20 acc_train: 72.14285714285714% acc_test: 83.33333333333334%\n",
            "Epoch: 21 acc_train: 72.14285714285714% acc_test: 83.33333333333334%\n",
            "Epoch: 22 acc_train: 74.28571428571429% acc_test: 81.25%\n",
            "Epoch: 23 acc_train: 75.0% acc_test: 83.33333333333334%\n",
            "Epoch: 24 acc_train: 74.28571428571429% acc_test: 83.33333333333334%\n",
            "Epoch: 25 acc_train: 75.0% acc_test: 83.33333333333334%\n",
            "Epoch: 26 acc_train: 75.0% acc_test: 83.33333333333334%\n",
            "Epoch: 27 acc_train: 75.0% acc_test: 85.41666666666666%\n",
            "Epoch: 28 acc_train: 75.0% acc_test: 83.33333333333334%\n",
            "Epoch: 29 acc_train: 73.57142857142858% acc_test: 83.33333333333334%\n",
            "Epoch: 30 acc_train: 75.0% acc_test: 83.33333333333334%\n",
            "Epoch: 31 acc_train: 74.28571428571429% acc_test: 83.33333333333334%\n",
            "Epoch: 32 acc_train: 75.71428571428571% acc_test: 83.33333333333334%\n",
            "Epoch: 33 acc_train: 75.71428571428571% acc_test: 81.25%\n",
            "Epoch: 34 acc_train: 75.71428571428571% acc_test: 83.33333333333334%\n",
            "Epoch: 35 acc_train: 75.71428571428571% acc_test: 81.25%\n",
            "Epoch: 36 acc_train: 75.71428571428571% acc_test: 85.41666666666666%\n",
            "Epoch: 37 acc_train: 75.0% acc_test: 81.25%\n",
            "Epoch: 38 acc_train: 76.42857142857142% acc_test: 81.25%\n",
            "Epoch: 39 acc_train: 75.0% acc_test: 81.25%\n",
            "Epoch: 40 acc_train: 75.71428571428571% acc_test: 79.16666666666666%\n",
            "Epoch: 41 acc_train: 74.28571428571429% acc_test: 83.33333333333334%\n",
            "Epoch: 42 acc_train: 76.42857142857142% acc_test: 81.25%\n",
            "Epoch: 43 acc_train: 76.42857142857142% acc_test: 81.25%\n",
            "Epoch: 44 acc_train: 75.71428571428571% acc_test: 77.08333333333334%\n",
            "Epoch: 45 acc_train: 76.42857142857142% acc_test: 81.25%\n",
            "Epoch: 46 acc_train: 73.57142857142858% acc_test: 81.25%\n",
            "Epoch: 47 acc_train: 75.71428571428571% acc_test: 81.25%\n",
            "Epoch: 48 acc_train: 76.42857142857142% acc_test: 81.25%\n",
            "Epoch: 49 acc_train: 75.71428571428571% acc_test: 83.33333333333334%\n",
            "Epoch: 50 acc_train: 77.14285714285715% acc_test: 83.33333333333334%\n",
            "Epoch: 51 acc_train: 76.42857142857142% acc_test: 81.25%\n",
            "Epoch: 52 acc_train: 75.71428571428571% acc_test: 79.16666666666666%\n",
            "Epoch: 53 acc_train: 76.42857142857142% acc_test: 81.25%\n",
            "Epoch: 54 acc_train: 76.42857142857142% acc_test: 81.25%\n",
            "Epoch: 55 acc_train: 77.85714285714286% acc_test: 81.25%\n",
            "Epoch: 56 acc_train: 75.0% acc_test: 81.25%\n",
            "Epoch: 57 acc_train: 76.42857142857142% acc_test: 81.25%\n",
            "Epoch: 58 acc_train: 75.0% acc_test: 81.25%\n",
            "Epoch: 59 acc_train: 75.0% acc_test: 81.25%\n",
            "Epoch: 60 acc_train: 75.71428571428571% acc_test: 79.16666666666666%\n",
            "Epoch: 61 acc_train: 78.57142857142857% acc_test: 79.16666666666666%\n",
            "Epoch: 62 acc_train: 79.28571428571428% acc_test: 79.16666666666666%\n",
            "Epoch: 63 acc_train: 77.85714285714286% acc_test: 79.16666666666666%\n",
            "Epoch: 64 acc_train: 78.57142857142857% acc_test: 79.16666666666666%\n",
            "Epoch: 65 acc_train: 80.0% acc_test: 83.33333333333334%\n",
            "Epoch: 66 acc_train: 78.57142857142857% acc_test: 79.16666666666666%\n",
            "Epoch: 67 acc_train: 78.57142857142857% acc_test: 79.16666666666666%\n",
            "Epoch: 68 acc_train: 79.28571428571428% acc_test: 79.16666666666666%\n",
            "Epoch: 69 acc_train: 78.57142857142857% acc_test: 81.25%\n",
            "Epoch: 70 acc_train: 78.57142857142857% acc_test: 79.16666666666666%\n",
            "Epoch: 71 acc_train: 78.57142857142857% acc_test: 79.16666666666666%\n",
            "Epoch: 72 acc_train: 78.57142857142857% acc_test: 81.25%\n",
            "Epoch: 73 acc_train: 79.28571428571428% acc_test: 79.16666666666666%\n",
            "Epoch: 74 acc_train: 79.28571428571428% acc_test: 79.16666666666666%\n",
            "Epoch: 75 acc_train: 79.28571428571428% acc_test: 79.16666666666666%\n",
            "Epoch: 76 acc_train: 78.57142857142857% acc_test: 81.25%\n",
            "Epoch: 77 acc_train: 75.0% acc_test: 83.33333333333334%\n",
            "Epoch: 78 acc_train: 77.14285714285715% acc_test: 81.25%\n",
            "Epoch: 79 acc_train: 80.0% acc_test: 81.25%\n",
            "Epoch: 80 acc_train: 76.42857142857142% acc_test: 79.16666666666666%\n",
            "Epoch: 81 acc_train: 75.71428571428571% acc_test: 79.16666666666666%\n",
            "Epoch: 82 acc_train: 77.85714285714286% acc_test: 81.25%\n",
            "Epoch: 83 acc_train: 79.28571428571428% acc_test: 79.16666666666666%\n",
            "Epoch: 84 acc_train: 77.14285714285715% acc_test: 79.16666666666666%\n",
            "Epoch: 85 acc_train: 79.28571428571428% acc_test: 79.16666666666666%\n",
            "Epoch: 86 acc_train: 76.42857142857142% acc_test: 79.16666666666666%\n",
            "Epoch: 87 acc_train: 77.14285714285715% acc_test: 79.16666666666666%\n",
            "Epoch: 88 acc_train: 79.28571428571428% acc_test: 81.25%\n",
            "Epoch: 89 acc_train: 75.0% acc_test: 79.16666666666666%\n",
            "Epoch: 90 acc_train: 77.14285714285715% acc_test: 81.25%\n",
            "Epoch: 91 acc_train: 80.0% acc_test: 83.33333333333334%\n",
            "Epoch: 92 acc_train: 79.28571428571428% acc_test: 77.08333333333334%\n",
            "Epoch: 93 acc_train: 79.28571428571428% acc_test: 77.08333333333334%\n",
            "Epoch: 94 acc_train: 80.71428571428572% acc_test: 77.08333333333334%\n",
            "Epoch: 95 acc_train: 80.0% acc_test: 81.25%\n",
            "Epoch: 96 acc_train: 77.14285714285715% acc_test: 83.33333333333334%\n",
            "Epoch: 97 acc_train: 77.85714285714286% acc_test: 81.25%\n",
            "Epoch: 98 acc_train: 78.57142857142857% acc_test: 79.16666666666666%\n",
            "Epoch: 99 acc_train: 77.85714285714286% acc_test: 81.25%\n",
            "Epoch: 100 acc_train: 77.14285714285715% acc_test: 79.16666666666666%\n",
            "Epoch: 101 acc_train: 77.85714285714286% acc_test: 81.25%\n",
            "Epoch: 102 acc_train: 81.42857142857143% acc_test: 79.16666666666666%\n",
            "Epoch: 103 acc_train: 81.42857142857143% acc_test: 83.33333333333334%\n",
            "Epoch: 104 acc_train: 80.71428571428572% acc_test: 81.25%\n",
            "Epoch: 105 acc_train: 75.71428571428571% acc_test: 79.16666666666666%\n",
            "Epoch: 106 acc_train: 77.14285714285715% acc_test: 81.25%\n",
            "Epoch: 107 acc_train: 80.71428571428572% acc_test: 77.08333333333334%\n",
            "Epoch: 108 acc_train: 79.28571428571428% acc_test: 79.16666666666666%\n",
            "Epoch: 109 acc_train: 80.0% acc_test: 77.08333333333334%\n",
            "Epoch: 110 acc_train: 79.28571428571428% acc_test: 81.25%\n",
            "Epoch: 111 acc_train: 75.0% acc_test: 81.25%\n",
            "Epoch: 112 acc_train: 75.0% acc_test: 81.25%\n",
            "Epoch: 113 acc_train: 80.71428571428572% acc_test: 81.25%\n",
            "Epoch: 114 acc_train: 80.0% acc_test: 81.25%\n",
            "Epoch: 115 acc_train: 79.28571428571428% acc_test: 79.16666666666666%\n",
            "Epoch: 116 acc_train: 80.0% acc_test: 79.16666666666666%\n",
            "Epoch: 117 acc_train: 75.0% acc_test: 70.83333333333334%\n",
            "Epoch: 118 acc_train: 78.57142857142857% acc_test: 75.0%\n",
            "Epoch: 119 acc_train: 80.0% acc_test: 83.33333333333334%\n",
            "Epoch: 120 acc_train: 80.0% acc_test: 79.16666666666666%\n",
            "Epoch: 121 acc_train: 80.71428571428572% acc_test: 81.25%\n",
            "Epoch: 122 acc_train: 78.57142857142857% acc_test: 81.25%\n",
            "Epoch: 123 acc_train: 80.0% acc_test: 81.25%\n",
            "Epoch: 124 acc_train: 80.0% acc_test: 83.33333333333334%\n",
            "Epoch: 125 acc_train: 80.0% acc_test: 81.25%\n",
            "Epoch: 126 acc_train: 80.71428571428572% acc_test: 81.25%\n",
            "Epoch: 127 acc_train: 80.71428571428572% acc_test: 83.33333333333334%\n",
            "Epoch: 128 acc_train: 80.0% acc_test: 81.25%\n",
            "Epoch: 129 acc_train: 79.28571428571428% acc_test: 81.25%\n",
            "Epoch: 130 acc_train: 81.42857142857143% acc_test: 83.33333333333334%\n",
            "Epoch: 131 acc_train: 81.42857142857143% acc_test: 81.25%\n",
            "Epoch: 132 acc_train: 80.71428571428572% acc_test: 81.25%\n",
            "Epoch: 133 acc_train: 80.0% acc_test: 81.25%\n",
            "Epoch: 134 acc_train: 80.71428571428572% acc_test: 83.33333333333334%\n",
            "Epoch: 135 acc_train: 81.42857142857143% acc_test: 79.16666666666666%\n",
            "Epoch: 136 acc_train: 79.28571428571428% acc_test: 79.16666666666666%\n",
            "Epoch: 137 acc_train: 80.0% acc_test: 81.25%\n",
            "Epoch: 138 acc_train: 80.0% acc_test: 79.16666666666666%\n",
            "Epoch: 139 acc_train: 77.85714285714286% acc_test: 72.91666666666666%\n",
            "Epoch: 140 acc_train: 77.85714285714286% acc_test: 75.0%\n",
            "Epoch: 141 acc_train: 82.14285714285714% acc_test: 81.25%\n",
            "Epoch: 142 acc_train: 80.71428571428572% acc_test: 81.25%\n",
            "Epoch: 143 acc_train: 81.42857142857143% acc_test: 81.25%\n",
            "Epoch: 144 acc_train: 77.14285714285715% acc_test: 77.08333333333334%\n",
            "Epoch: 145 acc_train: 80.71428571428572% acc_test: 81.25%\n",
            "Epoch: 146 acc_train: 80.71428571428572% acc_test: 83.33333333333334%\n",
            "Epoch: 147 acc_train: 80.0% acc_test: 81.25%\n",
            "Epoch: 148 acc_train: 80.0% acc_test: 81.25%\n",
            "Epoch: 149 acc_train: 80.71428571428572% acc_test: 81.25%\n",
            "Epoch: 150 acc_train: 80.71428571428572% acc_test: 81.25%\n",
            "Epoch: 151 acc_train: 81.42857142857143% acc_test: 81.25%\n",
            "Epoch: 152 acc_train: 82.14285714285714% acc_test: 81.25%\n",
            "Epoch: 153 acc_train: 81.42857142857143% acc_test: 81.25%\n",
            "Epoch: 154 acc_train: 82.85714285714286% acc_test: 81.25%\n",
            "Epoch: 155 acc_train: 81.42857142857143% acc_test: 83.33333333333334%\n",
            "Epoch: 156 acc_train: 81.42857142857143% acc_test: 81.25%\n",
            "Epoch: 157 acc_train: 82.14285714285714% acc_test: 81.25%\n",
            "Epoch: 158 acc_train: 81.42857142857143% acc_test: 79.16666666666666%\n",
            "Epoch: 159 acc_train: 82.85714285714286% acc_test: 79.16666666666666%\n",
            "Epoch: 160 acc_train: 81.42857142857143% acc_test: 81.25%\n",
            "Epoch: 161 acc_train: 80.71428571428572% acc_test: 79.16666666666666%\n",
            "Epoch: 162 acc_train: 82.14285714285714% acc_test: 81.25%\n",
            "Epoch: 163 acc_train: 77.14285714285715% acc_test: 72.91666666666666%\n",
            "Epoch: 164 acc_train: 82.14285714285714% acc_test: 81.25%\n",
            "Epoch: 165 acc_train: 79.28571428571428% acc_test: 77.08333333333334%\n",
            "Epoch: 166 acc_train: 80.71428571428572% acc_test: 81.25%\n",
            "Epoch: 167 acc_train: 80.0% acc_test: 81.25%\n",
            "Epoch: 168 acc_train: 78.57142857142857% acc_test: 81.25%\n",
            "Epoch: 169 acc_train: 79.28571428571428% acc_test: 81.25%\n",
            "Epoch: 170 acc_train: 80.0% acc_test: 81.25%\n",
            "Epoch: 171 acc_train: 80.71428571428572% acc_test: 83.33333333333334%\n",
            "Epoch: 172 acc_train: 80.71428571428572% acc_test: 83.33333333333334%\n",
            "Epoch: 173 acc_train: 80.71428571428572% acc_test: 85.41666666666666%\n",
            "Epoch: 174 acc_train: 81.42857142857143% acc_test: 85.41666666666666%\n",
            "Epoch: 175 acc_train: 82.85714285714286% acc_test: 85.41666666666666%\n",
            "Epoch: 176 acc_train: 82.85714285714286% acc_test: 81.25%\n",
            "Epoch: 177 acc_train: 78.57142857142857% acc_test: 75.0%\n",
            "Epoch: 178 acc_train: 80.71428571428572% acc_test: 72.91666666666666%\n",
            "Epoch: 179 acc_train: 82.14285714285714% acc_test: 83.33333333333334%\n",
            "Epoch: 180 acc_train: 82.14285714285714% acc_test: 83.33333333333334%\n",
            "Epoch: 181 acc_train: 81.42857142857143% acc_test: 83.33333333333334%\n",
            "Epoch: 182 acc_train: 82.14285714285714% acc_test: 83.33333333333334%\n",
            "Epoch: 183 acc_train: 82.14285714285714% acc_test: 83.33333333333334%\n",
            "Epoch: 184 acc_train: 82.85714285714286% acc_test: 83.33333333333334%\n",
            "Epoch: 185 acc_train: 82.85714285714286% acc_test: 83.33333333333334%\n",
            "Epoch: 186 acc_train: 83.57142857142857% acc_test: 83.33333333333334%\n",
            "Epoch: 187 acc_train: 82.85714285714286% acc_test: 85.41666666666666%\n",
            "Epoch: 188 acc_train: 82.14285714285714% acc_test: 85.41666666666666%\n",
            "Epoch: 189 acc_train: 82.85714285714286% acc_test: 83.33333333333334%\n",
            "Epoch: 190 acc_train: 80.0% acc_test: 79.16666666666666%\n",
            "Epoch: 191 acc_train: 79.28571428571428% acc_test: 79.16666666666666%\n",
            "Epoch: 192 acc_train: 83.57142857142857% acc_test: 83.33333333333334%\n",
            "Epoch: 193 acc_train: 82.14285714285714% acc_test: 83.33333333333334%\n",
            "Epoch: 194 acc_train: 81.42857142857143% acc_test: 83.33333333333334%\n",
            "Epoch: 195 acc_train: 82.85714285714286% acc_test: 85.41666666666666%\n",
            "Epoch: 196 acc_train: 83.57142857142857% acc_test: 83.33333333333334%\n",
            "Epoch: 197 acc_train: 83.57142857142857% acc_test: 83.33333333333334%\n",
            "Epoch: 198 acc_train: 84.28571428571429% acc_test: 83.33333333333334%\n",
            "Epoch: 199 acc_train: 82.85714285714286% acc_test: 83.33333333333334%\n"
          ]
        }
      ],
      "source": [
        "from torch import optim\n",
        "\n",
        "# 交差エントロピー誤差関数\n",
        "loss_fnc = nn.CrossEntropyLoss()\n",
        "\n",
        "# 最適化アルゴリズム\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "for epoch in range(200):\n",
        "    # 訓練\n",
        "    net.train()  # 訓練モード\n",
        "    for data in loader_train:\n",
        "        data = data.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # 勾配の初期化\n",
        "        out = net(data)  # 順伝播により予測値を得る\n",
        "        loss = loss_fnc(out, data.y)  # 予測値と正解値から誤差を計算\n",
        "\n",
        "        loss.backward()  # 誤差からバックプロパゲーションにより勾配を計算\n",
        "        optimizer.step()  # 最適化アルゴリズムによりパラメータを更新\n",
        "\n",
        "    # 評価\n",
        "    net.eval()  # 評価モード\n",
        "    acc_train = eval(loader_train)\n",
        "    acc_test = eval(loader_test)\n",
        "    print(\"Epoch:\", epoch,\n",
        "          \"acc_train:\", str(acc_train*100) + \"%\",\n",
        "          \"acc_test:\", str(acc_test*100) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3rn17KTjLSr"
      },
      "source": [
        "## モデルの評価\n",
        "訓練済みのモデルを評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7K0lvg0nQL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8373ee2-288b-4c36-ace0-3dba8508d1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 83.33333333333334%\n"
          ]
        }
      ],
      "source": [
        "net.eval()  # 評価モード\n",
        "acc_test = eval(loader_test)\n",
        "print(\"accuracy:\", str(acc_test*100) + \"%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}